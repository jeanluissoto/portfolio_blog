---
title: "Face Recognition"
format:
  html:
    toc: true
    html-math-method: katex
    css: styles.css
    notebook-links: false
    code-links:
    - text: Code file
      icon: file-code
      href: https://www.kaggle.com/code/jeanluissotovidal/face-recognition
---
# About
# Face Detection

```python
def process_dataset(data_dir, output_dir):

    os.makedirs(output_dir, exist_ok=True)

    mtcnn = MTCNN(keep_all=True, device='cuda' if torch.cuda.is_available() else 'cpu')

    for person_name in os.listdir(data_dir):
        person_dir = os.path.join(data_dir, person_name)
        if os.path.isdir(person_dir):
            output_person_dir = os.path.join(output_dir, person_name)
            os.makedirs(output_person_dir, exist_ok=True)

            for image_name in os.listdir(person_dir):
                if image_name.lower().endswith(('.png', '.jpg', '.jpeg')):
                    image_path = os.path.join(person_dir, image_name)
                    image = Image.open(image_path).convert('RGB')  # Convert to RGB
                    results = mtcnn.detect(image)

                    if results is not None:
                        delimit, logit = results
                        
                        if delimit is not None and logit is not None:
                            best_logit = max(logit)
                            indices = [i for i, x in enumerate(logit) if x == best_logit]
                            best_face = [delimit[index] for index in indices]                            
                            if best_face is not None:
                                    #x, y, w, h = (best_face[0], best_face[1], best_face[2], best_face[3])
                                    x, y, w, h = (best_face[0][0], best_face[0][1], best_face[0][2], best_face[0][3])
                                    cropped_face = image.crop((x, y,w, h))
                                    face_path = os.path.join(output_person_dir, f"face_{image_name}")

                                # Save the cropped face image
                                    cropped_face.save(face_path)
                        else: ('face not found:{image_path}')
    print("Face detection and cropping completed.")
```

```python
dataset_folder = r'curated_data\train'
output_folder = r'Final_project\mtcnn_processed_images'
os.makedirs(output_folder, exist_ok=True)
process_dataset(dataset_folder, output_folder)

dataset_folder = r'\curated_data\val'
output_folder = r'\data\mtcnn_test'
os.makedirs(output_folder, exist_ok=True)
process_dataset(dataset_folder, output_folder)
```

# Face Recognition

```python
from torchvision import transforms
train_transform = transforms.Compose([
   transforms.Resize((160,160)),
   transforms.RandomAffine(degrees=15, translate=[0.1, 0.1]), 
    transforms.RandomHorizontalFlip(),
    transforms.ColorJitter(),
    transforms.RandomGrayscale(p=0.1),
    transforms.RandomVerticalFlip(p=0.1),
    transforms.ToTensor(),
    transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])
])
```

```python
from torchvision import datasets 
train_dataset = datasets.ImageFolder(root=r'data\Final_project\mtcnn_processed_images', transform=train_transform)
val_dataset = datasets.ImageFolder(root=r'data\Final_project\mtcnn_test', transform=val_transform)
```

```python
class_names = train_dataset.classes
class_idx_to_label = {i: class_name for i, class_name in enumerate(class_names)}
```

{{< embed _Face_Recognition.ipynb#code-1 echo=true >}}
{{< embed _Face_Recognition.ipynb#code-2 echo=true >}}

```python
train_loader = DataLoader(dataset=train_dataset, batch_size=8)
test_loader = DataLoader(dataset=test_dataset, batch_size=8)
```

## FaceNet

```python
facenet_model = InceptionResnetV1(pretrained='vggface2').eval().to(device)
```
# Transfer Learning: Neural Networks

```python
class FaceNetModel(nn.Module):
    def __init__(self):
        super().__init__()
        facenet_model = InceptionResnetV1(pretrained='vggface2',classify=True, num_classes=46).eval().to(device)
        base_facenet = list(facenet_model.children())[:-5]
        self.features = nn.Sequential(*base_facenet)
        
        self.avgpool_1a = facenet_model.avgpool_1a
        self.dropout = facenet_model.dropout
        self.last_linear = facenet_model.last_linear
        self.last_bn = facenet_model.last_bn
        

        self.lightweight_network = nn.Sequential(
            nn.Flatten(),
            nn.Linear(512, 46),
            #nn.ReLU(),
            #nn.SiLU(),
            #nn.Dropout(0.3),
            #nn.Linear(120, 46),

        )

    def forward(self, x):
        out = self.features(x)
        out = self.avgpool_1a(out)
        out = self.dropout(out)
        out = torch.flatten(out, 1)
        out = self.last_linear(out)
        out = self.last_bn(out)
        out = self.lightweight_network(out)
        
        
        return out
```

{{< embed _Face_Recognition.ipynb#code-3 echo=true >}}

```python
model = FaceNetModel().to(device)

for params in model.features.parameters():
    params.requires_grad = False

print("Done")
```
# Train and Testing

```python
def accuracy_fn(y_true, y_pred):
    correct = torch.eq(y_true, y_pred).sum().item() # torch.eq() calculates where two tensors are equal
    acc = (correct / len(y_pred)) * 100 
    return acc
```

```python
torch.cuda.empty_cache() 
#import torch.optim.adabound as AdaBound
from torch.optim.lr_scheduler import StepLR
from sklearn.metrics import confusion_matrix
import seaborn as sns
import torch.nn.functional as F

#conf_matrix_val = torch.zeros(len(class_names), len(class_names))

train_losses = []; train_accs = []
validation_losses = []; validation_accs = []

lr = 0.001
n_epochs = 4

loss_fn = nn.CrossEntropyLoss()
#optimizer = AdaBound(model.parameters(), lr=lr, weight_decay=5e-4)
optimizer = optim.AdamW(model.parameters(), lr=lr, weight_decay=1e-4)
#scheduler = StepLR(optimizer, step_size=2, gamma=0.5)

for epoch in range(n_epochs):
    # Training Loop
    model.train()
    train_loss, train_acc = 0, 0
    
    for x_batch, y_batch in train_loader:  
            
        x_batch = x_batch.to(device)
        y_batch = y_batch.to(device)
        
        y_logits = model(x_batch)
        loss = loss_fn(y_logits, y_batch)
        
        train_loss += loss.item()
        
        optimizer.zero_grad()
        loss.backward()    
        optimizer.step()
        
        #scheduler.step()
        
        y_pred = torch.softmax(y_logits, dim=1).argmax(dim=1) # turn logits -> pred probs -> pred labels
        train_acc += accuracy_fn(y_true=y_batch, y_pred=y_pred) 
        softmax_probs = F.softmax(y_logits, dim=1)
        print("Softmax Probabilities:", softmax_probs)
        #conf_matrix_tr += confusion_matrix(y_batch.cpu().numpy(), y_pred.cpu().numpy())
        
    train_loss /= len(train_loader)
    train_losses.append(train_loss)
    
    train_acc /= len(train_loader)
    train_accs.append(train_acc)
    
    # Validation Loop
    model.eval()
    validation_loss , validation_acc = 0, 0
    
    with torch.no_grad():
        for x_val, y_val in test_loader:

            x_val = x_val.to(device)
            y_val = y_val.to(device)

            y_logits = model(x_val)
            val_loss = loss_fn(y_logits, y_val)
            
            validation_loss += val_loss.item()
            
            y_pred = torch.softmax(y_logits, dim=1).argmax(dim=1)
            validation_acc += accuracy_fn(y_true=y_val, y_pred=y_pred) 
            
        #conf_matrix_val += confusion_matrix(y_val.cpu().numpy(), y_pred.cpu().numpy(),labels=range(46))
            
    validation_loss /= len(test_loader)
    validation_losses.append(validation_loss)
    
    validation_acc /= len(test_loader)
    validation_accs.append(validation_acc)
    
    
    # Print out what's happening every 10 epochs
    if epoch % 1 == 0:
        print(f"Epoch: {epoch} | Loss: {train_loss:.5f}, Accuracy: {train_acc:.2f}% | val loss: {validation_loss:.5f}, val acc: {validation_acc:.2f}%")
    
      
# Plot confusion matrices
#fig, axes = plt.subplots(1, 1, figsize=(16, 8))
#sns.heatmap(conf_matrix_val, annot=True, fmt='g', ax=axes[1], cmap="crest")
#axes.set_title("Validation Confusion Matrix")
#plt.show()
```
Epoch: 0 | Loss: 0.41232, Accuracy: 90.65% | val loss: 0.19045, val acc: 95.83%
Epoch: 1 | Loss: 0.42497, Accuracy: 89.71% | val loss: 0.53357, val acc: 86.90%
Epoch: 2 | Loss: 0.40867, Accuracy: 90.20% | val loss: 0.45900, val acc: 91.07%
Epoch: 3 | Loss: 0.49615, Accuracy: 88.73% | val loss: 0.38051, val acc: 91.07%


{{< embed _Face_Recognition.ipynb#code-4 echo=true >}}

# Application
```python
def predict_frame(frame, model, transform, device):

    pil_image = Image.fromarray(cv2.cvtColor(frame, cv2.COLOR_BGR2RGB))
    mtcnn = MTCNN(keep_all=True, device='cuda' if torch.cuda.is_available() else 'cpu')

    boxes, _ = mtcnn.detect(pil_image)

    if boxes is None:
        return None, None

    predictions = []

    for box in boxes:

        face = pil_image.crop((box[0], box[1], box[2], box[3]))

        face = transform(face).unsqueeze(0).to(device)

        with torch.no_grad():
            outputs = model(face)
            _, predicted = torch.max(outputs.data, 1)
            predictions.append(predicted.item())

    return boxes, predictions
```

```python
# Initialize webcam
import cv2
import torch
from PIL import Image
from facenet_pytorch import MTCNN

cap = cv2.VideoCapture(0)

while True:
    ret, frame = cap.read()
    if not ret:
        break

    boxes, predictions = predict_frame(frame, model, transform, device)

    if boxes is not None:
        for box, predicted_class_idx in zip(boxes, predictions):

            predicted_class_name =   class_idx_to_label[predicted_class_idx]

            cv2.rectangle(frame, (int(box[0]), int(box[1])), (int(box[2]), int(box[3])), (0, 255, 0), 2)
            cv2.putText(frame, predicted_class_name, (int(box[0]), int(box[1]-10)), cv2.FONT_HERSHEY_COMPLEX, 0.5, (0, 255, 0), 1)

    cv2.imshow('Video Feed', frame)

    if cv2.waitKey(1) & 0xFF == ord('q'):
        break

cap.release()

cv2.destroyAllWindows() 
mtcnn = MTCNN(keep_all=True, device="cpu")
```